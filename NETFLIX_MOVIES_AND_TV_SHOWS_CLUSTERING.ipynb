{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "O_i_v8NEhb9l"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shashank251225/NETFLIX-MOVIES-AND-TV-SHOWS-CLUSTERING/blob/main/NETFLIX_MOVIES_AND_TV_SHOWS_CLUSTERING.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m3ZuPKzbw8Jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - NETFLIX MOVIES AND TV SHOWS CLUSTERING\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Unsupervised\n",
        "##### **Contribution**    - Team\n",
        "##### **Team Member 1 -**\n",
        "##### **Team Member 2 -**\n",
        "##### **Team Member 3 -**\n",
        "##### **Team Member 4 -**"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ".Write the summary here within 500-600 words."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset consists of tv shows and movies available on Netflix as of 2019. The dataset is collected from Flixable which is a third-party Netflix search.\n",
        "\n",
        "In 2018, they released an interesting report which shows that the number of TV shows on Netflix has nearly tripled since 2010. The streaming serviceâ€™s number of movies has decreased by more than 2,000 titles since 2010, while its number of TV shows has nearly tripled. It will be interesting to explore what all other insights can be obtained from the same dataset.\n",
        "\n",
        "Integrating this dataset with other external datasets such as IMDB ratings, rotten tomatoes can also provide many interesting findings"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Define Your Business Objective?**"
      ],
      "metadata": {
        "id": "PH-0ReGfmX4f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exploratory Data Analysis\n",
        "\n",
        "Understanding what type content is available in different countries\n",
        "\n",
        "Is Netflix has increasingly focusing on TV rather than movies in recent years.\n",
        "\n",
        "Clustering similar content by matching text-based featuresAnswer Here."
      ],
      "metadata": {
        "id": "PhDvGCAqmjP1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attribute **Information**\n",
        "show_id : Unique ID for every Movie / Tv Show\n",
        "\n",
        "type : Identifier - A Movie or TV Show\n",
        "\n",
        "title : Title of the Movie / Tv Show\n",
        "\n",
        "director : Director of the Movie\n",
        "\n",
        "cast : Actors involved in the movie / show\n",
        "\n",
        "country : Country where the movie / show was produced\n",
        "\n",
        "date_added : Date it was added on Netflix\n",
        "\n",
        "release_year : Actual Releaseyear of the movie / show\n",
        "\n",
        "rating : TV Rating of the movie / show\n",
        "\n",
        "duration : Total Duration - in minutes or number of seasons\n",
        "\n",
        "listed_in : Genere\n",
        "\n",
        "description: The Summary description"
      ],
      "metadata": {
        "id": "tOShZfqw-g5M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.mlab as mlab\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from textblob import TextBlob\n",
        "import scipy.stats as stats\n",
        "\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "   \n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a total of 7787 movies / TV shows in this data set collected from AlmaBetter\n",
        "\n",
        "Each row contains the following information: type (Movie or TV Show), title, director, cast, country, rating (ex. PG, PG-13, R, etc.), listed_in (genre), and plot description."
      ],
      "metadata": {
        "id": "3jlBfuao6Z-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "l4ogucF15TXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "netflix = pd.read_csv('/content/drive/MyDrive/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv')\n",
        "df = netflix.copy()\n"
      ],
      "metadata": {
        "id": "mTs2vq7n5t1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### INFORMATION: COLUMN NAMES, MISSING VALUES AND DATA TYPES"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "COLUMNS"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "3vnfW_83x6or"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DATASET SUMMARY"
      ],
      "metadata": {
        "id": "gN_NcuXMyXgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe(include = 'O').T"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe().T"
      ],
      "metadata": {
        "id": "lvXnG8rK0GiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MISSING VALUES"
      ],
      "metadata": {
        "id": "RNTY2Tp70Uxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "h9QAbyAK0Y8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#HANDLING MISSING VALUES\n",
        "df['director'].fillna('unknown', inplace=True)\n",
        "df['cast'].fillna('unknown', inplace=True)\n",
        "df['country'].fillna('unknown', inplace=True)\n",
        "df['date_added'].fillna('unknown', inplace=True)\n",
        "df['rating'].fillna('unknown', inplace=True)\n",
        "     "
      ],
      "metadata": {
        "id": "5Pd5RpXT0bsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CONVERTING 'date_added' to datetime format"
      ],
      "metadata": {
        "id": "ldKz7OZ80jyX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime as dt"
      ],
      "metadata": {
        "id": "JPtwVSXK0ggL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def date(row):\n",
        "  a = row['date_added']\n",
        "  if 'unknown' not in a:\n",
        "    month = a.split()[0]\n",
        "    year = a.split()[-1]\n",
        "    day = a.split()[1].split(',')[0]\n",
        "    a = dt.datetime.strptime(f'{month[:3]} {day} {year}' , '%b %d %Y')\n",
        "    return a\n",
        "  else:\n",
        "    return np.nan\n",
        "\n",
        "df['date_added'] = df.apply(lambda x: date(x), axis=1)"
      ],
      "metadata": {
        "id": "tAtCI1Lg0xcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FEATURE ENGINEERING LISTED_IN \n",
        "FEATURE ENGINEERING LISTED_IN\n",
        "Genres are extracted and re-defined accordingly. TV and Movie genres are separately defined. These are combined. Topics like International TV Shows are removed as it could bring in a bias by displaying content in reference to American movies."
      ],
      "metadata": {
        "id": "HiQAcs7k0_nH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = df.copy()"
      ],
      "metadata": {
        "id": "_Q1Rn6p81A4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a['listed_in'] = a.listed_in.apply(lambda row: row.split(', '))\n",
        "a.explode('listed_in')['listed_in'].unique()\n",
        "     "
      ],
      "metadata": {
        "id": "doP_f-PL1Kkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "replacing_genre = {'International TV Shows': '-',\n",
        "                   'TV Dramas': 'Drama',\n",
        "                   'TV Sci-Fi & Fantasy': 'SciFiFantasy',\n",
        "                   'Dramas': 'Drama' ,\n",
        "                   'International Movies': '-',\n",
        "                   'Horror Movies': 'Horror',\n",
        "                   'Action & Adventure': 'ActionAdventure',\n",
        "                   'Independent Movies': '-',\n",
        "                   'Sci-Fi & Fantasy': 'SciFiFantasy',\n",
        "                  'TV Mysteries': 'Mystery'       ,\n",
        "                  'Thrillers': 'Thriller',\n",
        "                   'Crime TV Shows': 'Crime',\n",
        "                   'Docuseries': 'Documentary',\n",
        "                  'Documentaries': 'Documentary', 'Sports Movies': 'Sports',\n",
        "                   'Comedies':'Comedy',\n",
        "                   'Anime Series': 'Anime',\n",
        "                  'Reality TV': 'Reality',\n",
        "                   'TV Comedies': 'Comedy',\n",
        "                   'Romantic Movies': 'Romance',\n",
        "                  'Romantic TV Shows': 'Romance', \n",
        "                   'Science & Nature TV': 'Science',\n",
        "                   'Movies': '-',\n",
        "                  'British TV Shows': '-',\n",
        "                   'Korean TV Shows': '-',\n",
        "                   'Music & Musicals': 'Music',\n",
        "                  'LGBTQ Movies': 'LGBTQ',\n",
        "                   'Faith & Spirituality': 'Spirituality', \n",
        "                   \"Kids' TV\": 'Kids',\n",
        "                  'TV Action & Adventure': 'ActionAdventure',\n",
        "                   'Spanish-Language TV Shows': '-',\n",
        "                  'Children & Family Movies': 'Family', \n",
        "                   'TV Shows': '-',\n",
        "                   'Classic Movies': 'Classic',\n",
        "                  'Cult Movies': 'Cult',\n",
        "                   'TV Horror': 'Horror',\n",
        "                   'Stand-Up Comedy & Talk Shows':'Comedy, TalkShow',\n",
        "                  'Teen TV Shows': 'Teen', 'Stand-Up Comedy':'Comedy', \n",
        "                   'Anime Features':'Anime',\n",
        "                  'TV Thrillers': 'Thriller',\n",
        "                   'Classic & Cult TV':'Classic, Cult'}"
      ],
      "metadata": {
        "id": "3zLFiB6d1PXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def genre_replace(row, replacing_genre = replacing_genre):\n",
        "  new_text = []\n",
        " \n",
        "  for word in (row['listed_in']):\n",
        "    if word in replacing_genre:\n",
        "      if '-' not in replacing_genre[word]:\n",
        "       new_text.append(replacing_genre[word])\n",
        "    else:\n",
        "      print(word, 'not present in dictionary')\n",
        "\n",
        "  return(', '.join(new_text))\n",
        "  \n",
        "\n",
        "df['Genres'] = a.apply(lambda row: genre_replace(row),axis=1)\n",
        "df['Genres'] = df['Genres'].apply(lambda row: row.split(', '))\n",
        "     "
      ],
      "metadata": {
        "id": "RNtbFfUO1ewu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(df.explode('Genres')['Genres'].unique())"
      ],
      "metadata": {
        "id": "4OH5kp-51hST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1 EXPLORATORY DATA ANALYSIS\n",
        "Histogram plot of release dates of shows/movies on Netflix"
      ],
      "metadata": {
        "id": "vAZofvUA1tVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = plt.figure(figsize=(14,8))\n",
        "a = sns.displot(x='release_year', hue = 'type', data = df, kind = 'hist', height=10, aspect=2, bins = 50, multiple='dodge', palette='GnBu')\n",
        "a = plt.xticks(ticks= np.arange(df.release_year.min(),df.release_year.max()+1,1), rotation = 90)\n",
        "a = plt.title('Distribution of Movie/Shows Release Dates')\n",
        "a = plt.xlabel('Release Year')"
      ],
      "metadata": {
        "id": "VAirA6UL1oJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OBSERVATIONS :\n",
        "Most Movies streaming on the platform were released after 2010\n",
        "Most TV Shows streaming on the platform were released after 2015\n",
        "The year 2017 had highest number of Movie and TV show releases on the platform"
      ],
      "metadata": {
        "id": "Ger6jn3y16uu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['date_added'].dt.year.max()"
      ],
      "metadata": {
        "id": "xD9-tb9d1znz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Histogram plot of dates of number of shows/movies added by the Streaming giant"
      ],
      "metadata": {
        "id": "6F3_Uf_E2Ezp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = plt.figure(figsize=(14,8))\n",
        "a = sns.displot(x= df['date_added'], hue = 'type', data = df, kind = 'hist', height=10, aspect=2, bins = 50, multiple='dodge', palette='GnBu')\n",
        "a = plt.title('Number of Movie/Shows Added by Year')\n",
        "a = plt.xlabel('Added Year')"
      ],
      "metadata": {
        "id": "KY0IuLRR2BGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OBSERVATIONS:\n",
        "Netflix began adding videos to the platform from 2008\n",
        "The streaming giant started aggressively adding movies and TV shows from 2017\n",
        "More movies are added as compared to TV shows"
      ],
      "metadata": {
        "id": "RMLkvqgo2uKv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Type of Videos on Netflix"
      ],
      "metadata": {
        "id": "SRhxzaSp2xjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = plt.figure(figsize=(14,8))\n",
        "a = sns.countplot(x= df['type'], data = df,palette='icefire')\n",
        "a = plt.title('Types of Video Entertainment on Netflix ')\n",
        "a = plt.xlabel('Type')\n",
        "a = plt.ylabel('Number of Shows / Movies')\n",
        "     "
      ],
      "metadata": {
        "id": "rBNh4izS2zzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OBSERVATIONS :\n",
        "There are almost twice as many movies as TV shows on Netflix."
      ],
      "metadata": {
        "id": "SOogksis25_2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Different Ratings on the Platform"
      ],
      "metadata": {
        "id": "zAa4HWWB3BXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = plt.figure(figsize=(14,8))\n",
        "d = df.groupby('rating').agg({'show_id':'count'}).reset_index()\n",
        "a = sns.barplot(x= d['rating'], y=d['show_id'], palette='bone')\n",
        "a = plt.title('Ratings Distribution of Movies and Shows on Netflix')\n",
        "a = plt.xlabel('Ratings')\n",
        "a = plt.ylabel('Number of Videos')\n",
        "     "
      ],
      "metadata": {
        "id": "P03N4sMC22M7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OBSERVATIONS :\n",
        "Most content on Netflix is rated for Mature Audiences and over 14 years old"
      ],
      "metadata": {
        "id": "FjNKRg015s2c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Top 10 Countries that produced content on Netflix"
      ],
      "metadata": {
        "id": "ehZWGcC55w6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = df[df.country != 'unknown'].loc[:,['show_id','title','country', 'listed_in']].copy()\n",
        "countrylist = (a['country'].to_list())\n",
        "\n",
        "newlist = []\n",
        "for country in countrylist:\n",
        "  if ',' in country:\n",
        "    a = country.split(', ')\n",
        "    newlist = newlist + a\n",
        "  else:\n",
        "    newlist.append(country)\n",
        "\n",
        "country_list = pd.DataFrame({'country' : newlist, 'id':np.arange(0,len(newlist),1)})\n",
        "a = country_list.groupby('country').agg({'id':'count'}).sort_values('id', ascending= False).head(20).reset_index()\n",
        "plt.figure(figsize=(15,10))\n",
        "a = sns.barplot(y = 'country', x = 'id',data = a , palette='icefire')\n",
        "a = plt.title('Movies/Show numbers produced in Countries')\n",
        "a = plt.ylabel('Country')\n",
        "a = plt.xlabel('Number of Videos')\n",
        "top10countries = country_list.groupby('country').agg({'id':'count'}).sort_values('id', ascending= False).head(10).reset_index()['country'].unique()"
      ],
      "metadata": {
        "id": "9svQNKZF3FsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OBSERVATIONS :\n",
        "Top Countries in Netflix are:\n",
        "\n",
        "1.United States\n",
        "2.India 3.United Kingdom 4.Canada 5.France"
      ],
      "metadata": {
        "id": "pZNamYyN6iDt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Top Genres on Netflix"
      ],
      "metadata": {
        "id": "Rv4ZxlBd6pof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = df.loc[:,['show_id','title','country', 'Genres']].copy()\n",
        "a = a.explode('Genres')\n",
        "genre_list = a.Genres.unique()\n",
        "group = a.groupby('Genres').agg(count = ('show_id','count')).sort_values('count', ascending= False).reset_index()\n",
        "plt.figure(figsize=(15,10))\n",
        "_ = sns.barplot(y = 'Genres', x = 'count',data = group , palette='inferno')"
      ],
      "metadata": {
        "id": "VbhJxjGx54GT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top15genres = group.head(15)['Genres'].to_list()"
      ],
      "metadata": {
        "id": "xJI9_9NL6vKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OBSERVATIONS\n",
        "Top Genres in Netflix are:\n",
        "\n",
        "Drama\n",
        "2.Comedy 3.Documentary 4.Action and Adventure 5.Romance"
      ],
      "metadata": {
        "id": "fhheX2nb68rg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Frequent Directors on Netflix"
      ],
      "metadata": {
        "id": "ikMuMxhk7CP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a=df[df.director != 'unknown'].loc[:,['show_id','title','director', 'cast']].copy()\n",
        "director = (a['director'].to_list())\n",
        "\n",
        "newlist = []\n",
        "for genre in director:\n",
        "  if ',' in genre:\n",
        "    a = genre.split(', ')\n",
        "    newlist = newlist + a\n",
        "  else:\n",
        "    newlist.append(genre)\n",
        "country_list = pd.DataFrame({'Director' : newlist, 'id':np.arange(0,len(newlist),1)})\n",
        "a = country_list.groupby('Director').agg({'id':'count'}).sort_values('id', ascending= False).reset_index().head(15)\n",
        "plt.figure(figsize=(15,10))\n",
        "a = sns.barplot(y = 'Director', x = 'id',data = a , palette= 'copper')\n",
        "a = plt.title('Directors on Netflix')\n",
        "a = plt.ylabel('Director')\n",
        "a = plt.xlabel('Number of Movies/Shows Directed')\n",
        "top10directors = country_list.groupby('Director').agg({'id':'count'}).sort_values('id', ascending= False).reset_index().head(10)['Director'].to_list()"
      ],
      "metadata": {
        "id": "66aNq4n26_9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OBSERVATIONS\n",
        "Top Directors on Netflix are:\n",
        "\n",
        "1.Jan Suter\n",
        "2.Raul Campos 3.Marcus Raboy 4.Jay Karas 5.Cathy Garcia-Molina"
      ],
      "metadata": {
        "id": "5lH3XKLN7iY-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Frequent Cast on Netflix"
      ],
      "metadata": {
        "id": "XLBj6v0P7n4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a=df[df.cast != 'unknown'].loc[:,['show_id','title','director', 'cast']].copy()\n",
        "cast = (a['cast'].to_list())\n",
        "\n",
        "newlist = []\n",
        "for genre in cast:\n",
        "  if ',' in genre:\n",
        "    a = genre.split(', ')\n",
        "    newlist = newlist + a\n",
        "  else:\n",
        "    newlist.append(genre)\n",
        "country_list = pd.DataFrame({'Actor' : newlist, 'id':np.arange(0,len(newlist),1)})\n",
        "a = country_list.groupby('Actor').agg({'id':'count'}).sort_values('id', ascending= False).head(20).reset_index()\n",
        "plt.figure(figsize=(15,10))\n",
        "a = sns.barplot(y = 'Actor', x = 'id',data = a , palette= 'bone')\n",
        "a = plt.title('Directors on Netflix')\n",
        "a = plt.ylabel('Director')\n",
        "a = plt.xlabel('Number of Movies/Shows Directed')\n",
        "top10actors = country_list.groupby('Actor').agg({'id':'count'}).sort_values('id', ascending= False).head(20).reset_index()['Actor'].to_list()"
      ],
      "metadata": {
        "id": "o0Sw-QQl7Gi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3dJQOcae7vqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OBSERVATIONS\n",
        "Top Actors on Netflix are:\n",
        "\n",
        "1.Anupam Kher\n",
        "2.Shah Rukh Khan 3.Naseeruddin Shah 4.Om Puri 5.Akshay Kumar"
      ],
      "metadata": {
        "id": "KATT_bH-70h1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DFWzRHnO71lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Duration of Movies on Netflix"
      ],
      "metadata": {
        "id": "07Rhv2oF76uI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a=df[df['type']=='Movie'].loc[:,['show_id','duration']].copy()\n",
        "cast = (a['duration'].to_list())\n",
        "\n",
        "newlist = []\n",
        "for genre in cast:\n",
        "  newlist.append(int(genre.split(' ')[0]))\n",
        "country_list = pd.DataFrame({'Duration' : newlist, 'id':np.arange(0,len(newlist),1)})\n",
        "a = sns.displot(x= 'Duration',  data = country_list, kind = 'hist', height=10, aspect=2, bins = 50)\n",
        "a = plt.title('Distribution of Movie Duration')\n",
        "a = plt.xlabel('Minutes')\n",
        "a = plt.xticks(ticks = np.arange(0,country_list.Duration.max()+1,10))\n",
        "     "
      ],
      "metadata": {
        "id": "9bg1C9nF77sO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "OBSERVATIONS\n",
        "Most movies on Netflix have a duration range from 90 to 110 minutes"
      ],
      "metadata": {
        "id": "6wAoOjdD8D0O"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oHavNj6G7_QT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Duration of TV shows on Netflix"
      ],
      "metadata": {
        "id": "qZgSbKts8G22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a=df[df['type']!='Movie'].loc[:,['show_id','duration']].copy()\n",
        "cast = (a['duration'].to_list())\n",
        "\n",
        "newlist = []\n",
        "for genre in cast:\n",
        "  newlist.append(int(genre.split(' ')[0]))\n",
        "country_list = pd.DataFrame({'Seasons' : newlist, 'id':np.arange(0,len(newlist),1)})\n",
        "a = sns.displot(x= 'Seasons',  data = country_list, kind = 'hist', height=10, aspect=2)\n",
        "a = plt.title('TV Show Season Distribution')\n",
        "a = plt.xlabel('Seasons')\n",
        "a = plt.xticks(ticks = np.arange(country_list.Seasons.min(),country_list.Seasons.max()+1,1))"
      ],
      "metadata": {
        "id": "yClQufXZ8H19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OBSERVATIONS\n",
        "Most TV shows on Netflix have a span of 1 season only"
      ],
      "metadata": {
        "id": "ZD5rqrFS8PlI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 UNDERSTANDING CONTENT PRODUCED IN DIFFERENT COUNTRIES"
      ],
      "metadata": {
        "id": "NQjtZhSt_NuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = df[df['country'] != 'unknown country'].loc[:,['show_id','title','country','listed_in','Genres']].copy()"
      ],
      "metadata": {
        "id": "MpkTkgfI8Myi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to convert string type to list \n",
        "def operation(row,string):\n",
        "  return row[string].split(', ')\n",
        "a['listed_in'] = a.apply(lambda x: operation(x,'listed_in'), axis = 1)\n",
        "a['country'] = a.apply(lambda x: operation(x,'country'), axis = 1)"
      ],
      "metadata": {
        "id": "jbVfLr71_U1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#explodes the list of categories and genres in each row to individual rows\n",
        "a = a.explode('Genres').explode('country')"
      ],
      "metadata": {
        "id": "NH6-DLNW_Yvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = a.groupby(['country','Genres']).agg({'show_id':'count'}).reset_index() #table contains number of videos belonging to each genre from different count"
      ],
      "metadata": {
        "id": "jKZ5pj9i_dpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to return the indeces that contain the top10 countries with the top 10 genres viewed\n",
        "def selectindex(row, attribute, list):\n",
        "  b = row[attribute]\n",
        "  boolean = b in list\n",
        "  return boolean\n",
        "\n",
        "index = a.apply(lambda x: selectindex(x, 'country',top10countries), axis = 1)\n",
        "newindex = a[index].apply(lambda x: selectindex(x, 'Genres',top15genres), axis = 1)"
      ],
      "metadata": {
        "id": "e4TLZQtV_fzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newdata = a[index][newindex] #data that contains the number videos in the top 15 genres in the top 10 countries"
      ],
      "metadata": {
        "id": "66gBDUq3_f6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gyr = ['#ff00ff','#00bfff',\n",
        " '#7f0000','#483d8b','#f0e68c','#b03060','#ff7f50','#00ff00',\n",
        " '#ee82ee','#00ffff','#1e90ff','#9acd32','#00fa9a','#ffb6c1',\n",
        " '#8b008b','#b8860b','#00008b','#ffd700','#008000','#008b8b','#e0ffff',\n",
        " '#000000','#ff0000','#0000ff','#556b2f']"
      ],
      "metadata": {
        "id": "UEdrK4c__yYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = plt.figure(figsize=(20,10))\n",
        "sns.barplot(x = newdata['country'], y = newdata['show_id'], hue = newdata['Genres'], palette= sns.color_palette(gyr[:23]) )\n",
        "plt.title('Video Content produced in the top 10 Countries')\n",
        "plt.xlabel('COUNTRY')\n",
        "plt.ylabel('NUMBER OF SHOWS & MOVIES')"
      ],
      "metadata": {
        "id": "iI3DcjXC_toO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OBSERVATIONS :\n",
        "Drama is the most produced genre in a lot of Non-English speaking countries\n",
        "Comedy is the most produced genre in English speaking countries like United States of America and United Kingdom and Canada\n",
        "Drama and Comedy are the most produced genres in the top countries with exceptions of Japan and South Korea\n",
        "Japan is the biggest producer of Anime. Anime is also the most produced in genre in Japan\n",
        "Most South Korean content are from the Romance genre*\n",
        "Documentaries are mainly produced in United Kingdom and United States of America"
      ],
      "metadata": {
        "id": "s6BW1iW__4lO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 Has Netflix Been Focusing Increasingly on TV Shows as compared to movies"
      ],
      "metadata": {
        "id": "g_0LnH-GAfPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "content = df.loc[:,['title','date_added','release_year', 'duration', 'type']].dropna().copy()"
      ],
      "metadata": {
        "id": "_usWwnhz_uJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content['date_added'] = content['date_added'].dt.year"
      ],
      "metadata": {
        "id": "P3EVKgR8AkyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Assuming TV shows release new seasons every year, we'll be adding a duplicate of the show for each season.\n",
        "def season(row):\n",
        "  release = row['release_year']\n",
        "  added = row['date_added']\n",
        "\n",
        "  if row['type'] =='TV Show':\n",
        "    seasons = int(row['duration'][0])\n",
        "    b = []\n",
        "    for i in range(1, seasons+1):\n",
        "      if i ==1:\n",
        "        b.append(release)\n",
        "      else:\n",
        "        b.append(b[-1]-1)\n",
        "    a = b\n",
        "  else:\n",
        "    a = []\n",
        "    a.append(row['release_year'])      \n",
        "  \n",
        "  return a\n",
        "\n",
        "content['Released Years'] = content.apply(lambda x: season(x),axis=1)"
      ],
      "metadata": {
        "id": "Y0Z_PYxZAlLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compare = content.explode('Released Years')"
      ],
      "metadata": {
        "id": "kd8i4Q5VAlOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tv = compare[compare['type'] == 'TV Show'].groupby(['date_added']).agg(count = ('title','count')).reset_index()\n",
        "tv['type'] = 'TV'\n",
        "movie = compare[compare['type'] != 'TV Show'].groupby(['date_added']).agg(count = ('title','count')).reset_index()\n",
        "movie['type'] = 'Movie'"
      ],
      "metadata": {
        "id": "e9-TFzuDAlRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.concat([tv, movie], axis = 0)"
      ],
      "metadata": {
        "id": "GhiKKR-fAlUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (15,10))\n",
        "sns.barplot(x = 'date_added', y='count', hue = 'type', data = data)\n",
        "plt.title('MOVIE VS TV -  Trends')\n",
        "plt.xlabel('Date Added')\n",
        "_ = plt.ylabel('Number of TV. Seasons/Movies Signed')"
      ],
      "metadata": {
        "id": "A4pFLiQdAlYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OBSERVATIONS :\n",
        "The above graph depicts seasons of TV shows signed vs the movies signed\n",
        "This distinction gives contacts as TV shows require recurring investment for each seasons. So the TV numbers have been increased in accordance to the seasons. As they were considered as one entity earlier\n",
        "We can observe that TV shows signed have been higher than movies in 2016. While the the movies signed have been higher, it is blatantly visible that the TV shows signed per year is catching up to the movies signed by the year"
      ],
      "metadata": {
        "id": "PMFVRnWCBEDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tv = compare[compare['type'] == 'TV Show'].groupby(['Released Years']).agg(count = ('title','count')).reset_index()\n",
        "tv['type'] = 'TV'\n",
        "movie = compare[compare['type'] != 'TV Show'].groupby(['Released Years']).agg(count = ('title','count')).reset_index()\n",
        "movie['type'] = 'Movie'\n",
        "     "
      ],
      "metadata": {
        "id": "OYn_AM5bAlcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.concat([tv, movie], axis = 0)"
      ],
      "metadata": {
        "id": "zdTB0E0EAlfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (20,10))\n",
        "sns.barplot(x = 'Released Years', y='count', hue = 'type', data = data)\n",
        "plt.title('MOVIE VS TV -  Trends')\n",
        "plt.xlabel('Released Years')\n",
        "plt.xticks(rotation = 90)\n",
        "_ = plt.ylabel('Number of TV. Seasons/Movies Signed')"
      ],
      "metadata": {
        "id": "FGMkJeBLBNAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OBSERVATIONS\n",
        "The above graph depicts an estimation of seasonal TV show and Movie release date per each year. We make a general assumption that each show releases a season yearly which is often the case for most TV shows.\n",
        "There are certainly exemptions. Eg. Ozark was released in years 2017, 2018, 2020 and 2022. [ P.S: Brilliant slow burn. Do check it out! ]\n",
        "We can observe that TV releases outnumbering movies from 2010. It is a known fact that each show contains multiple episodes, hence we can confidently say that significant amount of video content is being enlisted under TV shows from 2010\n",
        "Years 2019, 2020 and 2021 see multi-episode TV seasons outnumbering movies signed per year"
      ],
      "metadata": {
        "id": "xSFM3iErBSuE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TEXT PREPROCESSING"
      ],
      "metadata": {
        "id": "-hiqoK7ZBdZn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This process involves converting the 'Description' of the show/movie into a machine interprettable form. Here we convert all text to lower case, remove punctuations and remove irrelevant words (so that only words that provide context are retained). Here similar words are unified to save memory and processing time as well. Individual words and group of words (n grams) are also collected to extract context related details."
      ],
      "metadata": {
        "id": "QbQ8vimGBiab"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FEATURE ENGINEERING LISTED_IN\n",
        "\n",
        "Genres are extracted and re-defined accordingly"
      ],
      "metadata": {
        "id": "J4joYJ2bBj5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = df.copy()"
      ],
      "metadata": {
        "id": "wjfD1hqHBgI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a['listed_in'] = a.listed_in.apply(lambda row: row.split(', '))\n",
        "a.explode('listed_in')['listed_in'].unique()\n",
        "     "
      ],
      "metadata": {
        "id": "JAuz-xJlE-Oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "replacing_genre = {'International TV Shows': '-',\n",
        "                   'TV Dramas': 'Drama',\n",
        "                   'TV Sci-Fi & Fantasy': 'SciFiFantasy',\n",
        "                   'Dramas': 'Drama' ,\n",
        "                   'International Movies': '-',\n",
        "                   'Horror Movies': 'Horror',\n",
        "                   'Action & Adventure': 'ActionAdventure',\n",
        "                   'Independent Movies': '-',\n",
        "                   'Sci-Fi & Fantasy': 'SciFiFantasy',\n",
        "                  'TV Mysteries': 'Mystery'       ,\n",
        "                  'Thrillers': 'Thriller',\n",
        "                   'Crime TV Shows': 'Crime',\n",
        "                   'Docuseries': 'Documentary',\n",
        "                  'Documentaries': 'Documentary', 'Sports Movies': 'Sports',\n",
        "                   'Comedies':'Comedy',\n",
        "                   'Anime Series': 'Anime',\n",
        "                  'Reality TV': 'Reality',\n",
        "                   'TV Comedies': 'Comedy',\n",
        "                   'Romantic Movies': 'Romance',\n",
        "                  'Romantic TV Shows': 'Romance', \n",
        "                   'Science & Nature TV': 'Science',\n",
        "                   'Movies': '-',\n",
        "                  'British TV Shows': '-',\n",
        "                   'Korean TV Shows': '-',\n",
        "                   'Music & Musicals': 'Music',\n",
        "                  'LGBTQ Movies': 'LGBTQ',\n",
        "                   'Faith & Spirituality': 'Spirituality', \n",
        "                   \"Kids' TV\": 'Kids',\n",
        "                  'TV Action & Adventure': 'ActionAdventure',\n",
        "                   'Spanish-Language TV Shows': '-',\n",
        "                  'Children & Family Movies': 'Family', \n",
        "                   'TV Shows': '-',\n",
        "                   'Classic Movies': 'Classic',\n",
        "                  'Cult Movies': 'Cult',\n",
        "                   'TV Horror': 'Horror',\n",
        "                   'Stand-Up Comedy & Talk Shows':'Comedy, TalkShow',\n",
        "                  'Teen TV Shows': 'Teen', 'Stand-Up Comedy':'Comedy', \n",
        "                   'Anime Features':'Anime',\n",
        "                  'TV Thrillers': 'Thriller',\n",
        "                   'Classic & Cult TV':'Classic, Cult'}"
      ],
      "metadata": {
        "id": "Z-Ffns5MBtXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def genre_replace(row, replacing_genre = replacing_genre):\n",
        "  new_text = []\n",
        " \n",
        "  for word in (row['listed_in']):\n",
        "    if word in replacing_genre:\n",
        "      if '-' not in replacing_genre[word]:\n",
        "       new_text.append(replacing_genre[word])\n",
        "    else:\n",
        "      print(word, 'not present in dictionary')\n",
        "\n",
        "  return(', '.join(new_text))\n",
        "  \n",
        "\n",
        "df['Genres'] = a.apply(lambda row: genre_replace(row),axis=1)\n",
        "df['Genres'] = df['Genres'].apply(lambda row: row.split(', '))"
      ],
      "metadata": {
        "id": "BAGUroLRBtaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(df.explode('Genres')['Genres'].unique())"
      ],
      "metadata": {
        "id": "P2g_9ukABtcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Genres']"
      ],
      "metadata": {
        "id": "_ODP0H0mB8ok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## COMBINING Top CAST, DIRECTOR and ListedIn into single text attributes\n",
        "\n",
        "We are extracting non-plot related text details about the movie/show like lead actors, director, country and categories it is being listed in on the platform"
      ],
      "metadata": {
        "id": "6_nAjwbBCB9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def combine(row):\n",
        "  text = ' '.join(row['Genres']).lower()\n",
        "  if row['cast'] != 'unknown':\n",
        "    cast_list = [actor.replace(' ', '') for actor in row['cast'].split(', ')]\n",
        "    if len(cast_list) < 5:\n",
        "      text = text + ' ' + ' '.join(cast_list[:]).lower()\n",
        "    else :\n",
        "      text = text + ' ' + ' '.join(cast_list[:5]).lower()\n",
        "  if row['director'] !='unknown':\n",
        "    director_list = ' '.join([director.replace(' ', '') for director in row['director'].split(', ')])\n",
        "    text = text + ' ' + director_list.lower()\n",
        "\n",
        "  return text\n",
        "df['Movie Deets'] = df.apply(lambda row: combine(row), axis = 1)"
      ],
      "metadata": {
        "id": "CV0jF9N-FKrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()['Movie Deets'].to_list()"
      ],
      "metadata": {
        "id": "jUZ1aUyzFUap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importing necessary libraries for text processing\n",
        "import nltk \n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string \n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "CImN6q6CFUeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "IIslfcPeFUjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "mO22b8KlFUnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "5a8m3OGzFU43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WORD TOKENIZATION\n",
        "\n",
        "Tokenization breaks the raw text into words, sentences called tokens. These tokens help in understanding the context or developing the model for the NLP. The tokenization helps in interpreting the meaning of the text by analyzing the sequence of the words."
      ],
      "metadata": {
        "id": "spGuLBPqFy2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string"
      ],
      "metadata": {
        "id": "kGoQkdwAFU8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizer(txt):  \n",
        "  tokens = word_tokenize(txt) \n",
        "  \n",
        "  return tokens\n",
        "df['Processed Description'] = df['description'].apply(tokenizer)\n",
        "df['Processed Movie Deets'] = df['Movie Deets'].apply(tokenizer)"
      ],
      "metadata": {
        "id": "1w74ZA-6FU_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PUNCTUATION REMOVAL"
      ],
      "metadata": {
        "id": "Yvv6PI0fGBx-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regular_punct = list(string.punctuation)\n",
        "def remove_punctuation(text,punct_list):\n",
        "  tokens = text\n",
        "  new_tok = []\n",
        "  for i in tokens:\n",
        "    word = i\n",
        "    for punctuation in list(string.punctuation):\n",
        "      word = word.replace(punctuation, ' ')\n",
        "    if word.replace(' ','').isalnum():\n",
        "      new_tok.append(word)\n",
        "\n",
        "  return ' '.join(new_tok).split(' ')\n",
        "\n",
        "df['Processed Description'] = df['Processed Description'].apply(lambda x : remove_punctuation(x, regular_punct))\n",
        "df['Processed Movie Deets'] = df['Processed Movie Deets'].apply(lambda x : remove_punctuation(x, regular_punct))\n"
      ],
      "metadata": {
        "id": "euL80tGBGFx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STOP WORD REMOVAL"
      ],
      "metadata": {
        "id": "hUIkULFPGN3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "def remove_stop(tokens, stopwords):\n",
        "  return [t.lower() for t in tokens if t.lower() not in stopwords]\n",
        "\n",
        "\n",
        "df['Processed Description'] = df['Processed Description'].apply(lambda x : remove_stop(x, stop_words))\n",
        "\n",
        "a = df.copy() #will be used for wordcloud later on\n",
        "     "
      ],
      "metadata": {
        "id": "sruOUFHnGPHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEMMING\n",
        "Stemming is the process of producing morphological variants of a root/base word. Stemming programs are commonly referred to as stemming algorithms or stemmers."
      ],
      "metadata": {
        "id": "S_KO5wTjGVIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ps = PorterStemmer()\n",
        "\n",
        "def stem_text(text):  \n",
        "  stemmed = []\n",
        "  for word in text:\n",
        "    stemmed.append(ps.stem(word))\n",
        "  return stemmed\n",
        "\n",
        "\n",
        "df['Processed Description'] = df['Processed Description'] .apply(lambda x : stem_text(x))"
      ],
      "metadata": {
        "id": "UEMvP2NGGXY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def joiner(text_list):\n",
        "  new = []\n",
        "  for text in text_list:\n",
        "    if text == ' ' or text == '' :\n",
        "      pass\n",
        "    else:\n",
        "      new.append(text)\n",
        "      \n",
        "  return ' '.join(new)\n",
        "\n",
        "df['Processed Description'] = df['Processed Description'] .apply(lambda x : joiner(x))\n",
        "df['Processed Movie Deets'] = df['Processed Movie Deets'].apply(lambda x : joiner(x))\n",
        "     "
      ],
      "metadata": {
        "id": "_0AQ1xtmGnZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[:,['description','Processed Description','Movie Deets',\t'Processed Movie Deets']].head(1).T"
      ],
      "metadata": {
        "id": "OR1KXQiDGy9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorizing Texts"
      ],
      "metadata": {
        "id": "_sjv8OgaG4j9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
      ],
      "metadata": {
        "id": "lBJl9b_iG6xd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_vectorizer = CountVectorizer(min_df = 10,max_df=0.5)\n",
        "count_description_vector = count_vectorizer.fit_transform(df['Processed Description'])\n",
        "\n",
        "count_movie_vector = count_vectorizer.fit_transform(df['Processed Movie Deets'])"
      ],
      "metadata": {
        "id": "b80DImyYG9Ka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_description_vector.shape"
      ],
      "metadata": {
        "id": "WR-bYLUGG9Np"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_movie_vector.shape"
      ],
      "metadata": {
        "id": "BU_NlCo0G9Qq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA()\n",
        "pca.fit(pd.concat([pd.DataFrame(count_description_vector.todense()), pd.DataFrame(count_movie_vector.todense())],axis =1))"
      ],
      "metadata": {
        "id": "2VcwkZqJHJop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exp_var_pca = pca.explained_variance_ratio_\n",
        "#\n",
        "# Cumulative sum of eigenvalues; This will be used to create step plot\n",
        "# for visualizing the variance explained by each principal component.\n",
        "#\n",
        "cum_sum_eigenvalues = np.cumsum(exp_var_pca)\n",
        "#\n",
        "# Create the visualization plot\n",
        "#\n",
        "plt.figure(figsize = (20,10))\n",
        "plt.bar(range(0,len(exp_var_pca)), exp_var_pca, alpha=0.5, align='center', label='Individual explained variance')\n",
        "plt.step(range(0,len(cum_sum_eigenvalues)), cum_sum_eigenvalues, where='mid',label='Cumulative explained variance')\n",
        "plt.ylabel('Explained variance ratio')\n",
        "plt.xlabel('Principal component index')\n",
        "plt.legend(loc='best')\n",
        "plt.tight_layout()\n",
        "plt.xticks(ticks = np.arange(0,2300,100))\n",
        "plt.axhline(y=0.75)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Z5OnX_d0HNon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorising the preprocessed attributes Movie Deets and Description has sum total of 2126 + 192 = 2318 dimensions. These dimensions will have to be reduced using PCA, which would result in loss of information. Alternatively, the two attributes can be used to model the content into topics using Latent Dirichlet Allocation. This would makes sure that all the topical information about video content are captured without putting any available information to waste."
      ],
      "metadata": {
        "id": "wv3BfbArHpCQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BAG OF WORDS WORDCLOUD GENERATION of CONTENT DESCRIPTION BASED ON GENRE"
      ],
      "metadata": {
        "id": "e_hOhELzHscZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud"
      ],
      "metadata": {
        "id": "VY4NuL5oHvG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "genres = df.explode('Genres').Genres.unique()"
      ],
      "metadata": {
        "id": "dcXQIXOCHyMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "copy =  a.explode('Genres').copy()\n",
        "a = copy.copy()\n",
        "a['Processed Description'] = a['Processed Description'] .apply(lambda x : joiner(x))"
      ],
      "metadata": {
        "id": "GYOplILDHyP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(35, 30))\n",
        "plt.suptitle(\"Word Cloud for Different Genres\", fontsize=18, y=0.5)\n",
        "\n",
        "for genre in top15genres:\n",
        "  if genre != '':\n",
        "    \n",
        "    count_description_vectorizer = CountVectorizer(ngram_range = (1,1), min_df = 5, max_df = 0.7)\n",
        "    # fit the count vectorizer using the text data\n",
        "    count_description_vectorizer.fit(a[a.Genres == genre]['Processed Description'])\n",
        "    dictionary = count_description_vectorizer.vocabulary_.items()\n",
        "\n",
        "    vocab = []\n",
        "    count_of_vocab = []\n",
        "    # Iterate through each vocab and count append the value to designated lists\n",
        "    for key, value in dictionary:\n",
        "        vocab.append(key)\n",
        "        count_of_vocab.append(value)\n",
        "\n",
        "    count_description_df = pd.DataFrame({'Token':vocab, 'Count':np.array(count_of_vocab)}).sort_values('Count', ascending= False)\n",
        "    comment_words = ' '.join(count_description_df['Token'].to_list())\n",
        "    \n",
        "    wordcloud = WordCloud(width = 1200, height = 800,\n",
        "                    background_color ='black',\n",
        "                    min_font_size = 10, random_state = 33).generate(comment_words)\n",
        "                    # plot the WordCloud image \n",
        "    plt.figure(figsize = (12, 8), facecolor = None)                     \n",
        "    plt.imshow(wordcloud)\n",
        "    plt.axis(\"off\")\n",
        "    plt.tight_layout(pad = 0)\n",
        "    plt.title(genre.upper())\n",
        "    \n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "eqgpahcQHyTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BIGRAM BAG OF WORDS WORDCLOUD GENERATION of CONTENT DESCRIPTION BASED ON GENRE"
      ],
      "metadata": {
        "id": "3yvsnA-ZIRSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = copy.copy()\n",
        "a['Processed Description'] = a['Processed Description'] .apply(lambda x : joiner(x))"
      ],
      "metadata": {
        "id": "lhU0hrarHygl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(35, 30))\n",
        "plt.suptitle(\"Word Cloud for Different Genres\", fontsize=18, y=0.5)\n",
        "\n",
        "for genre in top15genres:\n",
        "  if genre != '':\n",
        "    \n",
        "    count_description_vectorizer = CountVectorizer(ngram_range = (2,2), min_df = 1, max_df = 0.7)\n",
        "    # fit the count vectorizer using the text data\n",
        "    count_description_vectorizer.fit(a[a.Genres == genre]['Processed Description'])\n",
        "    dictionary = count_description_vectorizer.vocabulary_.items()\n",
        "\n",
        "    vocab = []\n",
        "    count_of_vocab = []\n",
        "    # Iterate through each vocab and count append the value to designated lists\n",
        "    for key, value in dictionary:\n",
        "        vocab.append(key)\n",
        "        count_of_vocab.append(value)\n",
        "\n",
        "\n",
        "    count_description_df = pd.DataFrame({'Token':vocab, 'Count':np.array(count_of_vocab)}).sort_values('Count', ascending = False)\n",
        "    comment_words = ' '.join(count_description_df['Token'].apply(lambda row: ''.join(row.split(' '))).to_list())\n",
        "        \n",
        "    wordcloud = WordCloud(width = 1200, height = 800,\n",
        "                    background_color ='black',\n",
        "                    min_font_size = 10, random_state = 33).generate(comment_words)\n",
        "                    # plot the WordCloud image \n",
        "    plt.figure(figsize = (12, 8), facecolor = None)                     \n",
        "    plt.imshow(wordcloud)\n",
        "    plt.axis(\"off\")\n",
        "    plt.tight_layout(pad = 0)\n",
        "    plt.title(genre.upper())\n",
        "    \n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "8NK5CG30IWAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing dataset for clustering"
      ],
      "metadata": {
        "id": "h7MF1gh3Io6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clustering_df = df[['type', 'duration','rating','release_year']]"
      ],
      "metadata": {
        "id": "iDeD_-F5IrIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoding TV and Movie ratings into one based on their age reccomendations\n",
        "\n",
        "Guidelines:\n",
        "\n",
        "Amazon: Maturity Rating Guidelines\n",
        "\n",
        "Netflix: Maturity Rating Guidelines"
      ],
      "metadata": {
        "id": "gxSJKOxSI2QB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def topick(row):\n",
        "  older_kids = ['TV-PG', 'PG', 'TV-Y7', 'TV-Y7-FV']\n",
        "  kids = ['TV-G', 'G', 'TV-Y']\n",
        "  teens = ['PG-13','TV-14']\n",
        "  adult = ['R', 'NC-17', 'TV-MA', 'UR', 'NR']\n",
        "\n",
        "  rating = 'Others'\n",
        "  for column in older_kids + kids + teens + adult:\n",
        "    if row[column] == 1:\n",
        "      if column in older_kids:\n",
        "        rating = 'Older Kids'\n",
        "      elif column in kids :\n",
        "        rating = 'Kids'\n",
        "      elif column in teens :\n",
        "        rating = 'Teens'\n",
        "      elif column in adult:\n",
        "        rating = 'Adult'\n",
        "      \n",
        "  return rating\n",
        "\n",
        "ratings = pd.get_dummies(clustering_df['rating']).drop('unknown',axis=1).apply(lambda x: topick(x),axis=1)\n",
        "     "
      ],
      "metadata": {
        "id": "oIzEwSx2ItcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ENCODING TYPE, RATING"
      ],
      "metadata": {
        "id": "NDbb_8IoI_UQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clustering_df = pd.concat([clustering_df.drop(['type','rating'],axis =1),pd.get_dummies(ratings).drop('Others', axis=1), pd.get_dummies(clustering_df['type'])], axis =1)\n",
        "     "
      ],
      "metadata": {
        "id": "GA0cGOtgItfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clustering_df"
      ],
      "metadata": {
        "id": "GYDcRLtRJ7-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PROCESSING MOVIE DURATION"
      ],
      "metadata": {
        "id": "x5YnN_l0KA3C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting season numbers and movie duration as separate columns"
      ],
      "metadata": {
        "id": "5R8Opjp7KGFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def duration(row,column):\n",
        "  text = row['duration'].split()\n",
        "  if column == 'show':\n",
        "    if row['Movie'] == 0:\n",
        "      return int(text[0])\n",
        "    else:\n",
        "      return 0\n",
        "  if column == 'movie':\n",
        "    if row['Movie'] ==1:\n",
        "      return int(text[0])\n",
        "    else:\n",
        "      return 0\n",
        "clustering_df['show_duration'] = clustering_df.apply(lambda x: duration(x,'show'),axis =1)\n",
        "\n",
        "clustering_df['movie_duration'] = clustering_df.apply(lambda x: duration(x,'movie'), axis=1)"
      ],
      "metadata": {
        "id": "CwjdXDYwKDEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clustering_df.drop('duration', axis = 1, inplace=True)"
      ],
      "metadata": {
        "id": "i983tRmtKJ87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Binning Release Years based on decades"
      ],
      "metadata": {
        "id": "60WhD1pQKQot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(clustering_df['release_year']).unique()"
      ],
      "metadata": {
        "id": "yLf_AtupKNNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bin_release_year(row):\n",
        "  r_year = row['release_year']\n",
        "  for yr in np.arange(2030,1910,-10):\n",
        "    if r_year >= yr:\n",
        "      return (yr)\n",
        "\n",
        "clustering_df['release_year_bins'] =(clustering_df.apply(lambda x: bin_release_year(x), axis=1))"
      ],
      "metadata": {
        "id": "sEsRVmrSKY0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clustering_df"
      ],
      "metadata": {
        "id": "TZXKME8kKcVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer\n",
        "\n",
        "\n",
        "import matplotlib.cm as cm\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "from sklearn.cluster import KMeans , AgglomerativeClustering, DBSCAN\n",
        "from sklearn.cluster import AffinityPropagation\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.preprocessing import OneHotEncoder\n"
      ],
      "metadata": {
        "id": "cEFcLwayUOZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TOPIC MODELING using GENSIM"
      ],
      "metadata": {
        "id": "WicIuNwoNSaO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_input = df['Processed Description'] + ' ' + df['Processed Movie Deets']"
      ],
      "metadata": {
        "id": "Hak1VhzDNU2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.ldamulticore import LdaMulticore\n",
        "from gensim.models.coherencemodel import CoherenceModel"
      ],
      "metadata": {
        "id": "EXSkWEydNdb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.corpora import Dictionary\n",
        "\n",
        "gensim_paragraphs = (topic_input).apply(lambda row: row.split(' '))\n",
        "dict_gensim_para = Dictionary(gensim_paragraphs) # dictionary of corpus\n",
        "dict_gensim_para.filter_extremes(no_below=5, no_above=0.6)\n",
        "\n",
        "bow_gensim_para = [dict_gensim_para.doc2bow(paragraph) for paragraph in gensim_paragraphs] #creating corpus"
      ],
      "metadata": {
        "id": "H2yFE5gpNgWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TFIDF VECTOR"
      ],
      "metadata": {
        "id": "loBLinJUNkgA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import TfidfModel\n",
        "tfidf_gensim_para = TfidfModel(bow_gensim_para)\n",
        "vectors_gensim_para = tfidf_gensim_para[bow_gensim_para]"
      ],
      "metadata": {
        "id": "fDcrwUm_Nqql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda_para_model_n = []\n",
        "for n in (range(5, 16)):\n",
        "  lda_model = LdaMulticore(corpus=bow_gensim_para, id2word=dict_gensim_para,\n",
        "                            chunksize=1000, eta='auto', iterations=400,\n",
        "                            num_topics=n, passes=20, eval_every=None,\n",
        "                            random_state=42)\n",
        "  lda_coherence = CoherenceModel(model=lda_model, texts=gensim_paragraphs,\n",
        "                            dictionary=dict_gensim_para, coherence='c_npmi') \n",
        "  print(n,'|', lda_coherence.get_coherence())\n",
        "  lda_para_model_n.append((n, lda_model, lda_coherence.get_coherence()))"
      ],
      "metadata": {
        "id": "umRPa4NyNt3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda_df = pd.DataFrame(lda_para_model_n, columns=[\"n\", \"model\", \"coherence\"]).set_index(\"n\")[[\"coherence\"]]\n",
        "plt.figure(figsize=(16,9))\n",
        "_ = sns.lineplot(x='n', y='coherence', data=lda_df )\n",
        "_ = plt.xticks(ticks = np.arange(4,25,1))"
      ],
      "metadata": {
        "id": "vISkB-7HNxAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda_df = lda_df.reset_index()\n",
        "topic_nums = lda_df[lda_df['coherence']==lda_df['coherence'].max()]['n'].to_list()[0]\n",
        "topic_nums"
      ],
      "metadata": {
        "id": "Pd6ardbDOcLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda_gensim_para = LdaMulticore(corpus=bow_gensim_para, id2word=dict_gensim_para,\n",
        "                            chunksize=2000, eta='auto', iterations=400,\n",
        "                            num_topics= topic_nums , passes=20, eval_every=None,\n",
        "                            random_state=42)"
      ],
      "metadata": {
        "id": "N4lHZKehOgQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to display gensim topics\n",
        "def display_gensim_topics(model):\n",
        "  gen_list = lda_gensim_para.print_topics()\n",
        "  for i in range(0,len(gen_list)):\n",
        "    print('Topic: ',i+1,'\\n')\n",
        "    topics = gen_list[i][1]\n",
        "    for j in topics.split('+'):\n",
        "      print(j.split('\"')[1])\n",
        "\n",
        "    print(\"------\\n\")\n",
        "\n",
        "display_gensim_topics(lda_gensim_para)"
      ],
      "metadata": {
        "id": "NcXmWg7zOjiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyLDAvis"
      ],
      "metadata": {
        "id": "ELGNrCYCOnac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models"
      ],
      "metadata": {
        "id": "SHLEVtoEOrFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pyLDAvis.enable_notebook()\n",
        "p = pyLDAvis.gensim_models.prepare(lda_gensim_para, bow_gensim_para, dict_gensim_para)\n",
        "\n",
        "p"
      ],
      "metadata": {
        "id": "HPInWaicOt_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda = {}\n",
        "for i in range(0,lda_gensim_para.num_topics):\n",
        "  lda[f'Topic {i}'] = np.zeros(df.shape[0])"
      ],
      "metadata": {
        "id": "hZ9dnwScO0sZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, row in enumerate(lda_gensim_para[bow_gensim_para]):\n",
        "  for topic in row:\n",
        "    lda[f'Topic {topic[0]}'][i] = topic[1] "
      ],
      "metadata": {
        "id": "q6RiDoINO3Jo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model_df = pd.DataFrame(lda)"
      ],
      "metadata": {
        "id": "Zrv2YP3jO6US"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model_df"
      ],
      "metadata": {
        "id": "FZvFZt8QO8l6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def topic_input(row):\n",
        "  highest = 0\n",
        "  top = 'None'\n",
        "  for topic in topic_model_df.columns:\n",
        "    if row[topic] > highest:\n",
        "      highest = row[topic]\n",
        "      top = f'Topic {int(topic[-1])+1}'\n",
        "  return top\n",
        "\n",
        "topic = topic_model_df.apply(lambda x:topic_input(x),axis=1)"
      ],
      "metadata": {
        "id": "kNa_oZy2O8tA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CREATING WORD CLOUDS FOR TOPICS"
      ],
      "metadata": {
        "id": "7czD4TY_PA54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = copy.copy()\n",
        "a['Processed Description'] = a['Processed Description'] .apply(lambda x : joiner(x))\n",
        "a = pd.concat([a,pd.DataFrame(topic, columns = ['Topic'])],axis=1)"
      ],
      "metadata": {
        "id": "ISsOBtjDPGJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(35, 30))\n",
        "plt.suptitle(\"Word Cloud for Different Genres\", fontsize=18, y=0.5)\n",
        "\n",
        "for top in topic.unique():\n",
        "\n",
        "  count_description_vectorizer = CountVectorizer(ngram_range = (1,2), min_df = 5, max_df = 0.7)\n",
        "  # fit the count vectorizer using the text data\n",
        "  count_description_vectorizer.fit(a[a['Topic'] == top]['Processed Description'])\n",
        "  dictionary = count_description_vectorizer.vocabulary_.items()\n",
        "\n",
        "  vocab = []\n",
        "  count_of_vocab = []\n",
        "  # Iterate through each vocab and count append the value to designated lists\n",
        "  for key, value in dictionary:\n",
        "      vocab.append(key)\n",
        "      count_of_vocab.append(value)\n",
        "\n",
        "\n",
        "  count_description_df = pd.DataFrame({'Token':vocab, 'Count':np.array(count_of_vocab)}).sort_values('Count', ascending = False)\n",
        "  comment_words = ' '.join(count_description_df['Token'].apply(lambda row: ''.join(row.split(' '))).to_list())\n",
        "      \n",
        "  wordcloud = WordCloud(width = 1200, height = 800,\n",
        "                  background_color ='black',\n",
        "                  min_font_size = 10).generate(comment_words)\n",
        "                  # plot the WordCloud image \n",
        "  plt.figure(figsize = (12, 8), facecolor = None)                     \n",
        "  plt.imshow(wordcloud)\n",
        "  plt.axis(\"off\")\n",
        "  plt.tight_layout(pad = 0)\n",
        "  plt.title(top.upper())\n",
        "  \n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "gQWkBsLJPGQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Concatenating clustering_df with topic models"
      ],
      "metadata": {
        "id": "8KdRh281PSXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clustering_df"
      ],
      "metadata": {
        "id": "crS5gYpxPNAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SCALING"
      ],
      "metadata": {
        "id": "EmKwHo8WR9Ui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#vectorizer_clustering = pd.concat([clustering_df.drop(['release_year','Movie','TV Show'], axis=1), pd.get_dummies(clustering_df['release_year'])], a"
      ],
      "metadata": {
        "id": "Vr3niGC5PNDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer_clustering = pd.concat([clustering_df.drop(['release_year'], axis=1),], axis =1)"
      ],
      "metadata": {
        "id": "qs6hY2oXSDyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer_clustering"
      ],
      "metadata": {
        "id": "ei4IQwEqSIwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = MinMaxScaler()\n",
        "X_rescale = scaler.fit_transform(vectorizer_clustering)\n",
        "#dataset contains degree of belonging of document to each topic concatenated to scaled Dataframe \n",
        "X = pd.concat([pd.DataFrame(X_rescale,columns = vectorizer_clustering.columns ),(topic_model_df)],axis=1)"
      ],
      "metadata": {
        "id": "1w6euB4VSQUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "id": "XKZPZNWpSQZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = X.to_numpy()"
      ],
      "metadata": {
        "id": "pUDczgEWSamK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "id": "CySc8kVLSgaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performing Hopkins test for uniform distribution to check if clustering is possible on the dataset"
      ],
      "metadata": {
        "id": "PSneQfdLTEF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyclustertend"
      ],
      "metadata": {
        "id": "4Dsru35rTFqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numba==0.53"
      ],
      "metadata": {
        "id": "rEOBvIr7UpE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyclustertend import hopkins\n",
        "hopkins(X,150)"
      ],
      "metadata": {
        "id": "h0iYNc1DTJV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "id": "DLYmZseAUwK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CREATING DICTIONARY FOR REPORT"
      ],
      "metadata": {
        "id": "kfxgmnCGVEqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clustering_report = {\n",
        "    'Algorithm':[],\n",
        "    'Parameters':[],\n",
        "    'Clusters':[],\n",
        "    'Silhouette Coefficient':[],\n",
        "    'Davies-Bouldin Index': [],\n",
        "    'Calinski-Harbaz Score': []\n",
        "}"
      ],
      "metadata": {
        "id": "U-qWq1EZUwQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_report(algo_name, details,clusters, sil_score, db_index, ch_score):\n",
        "  data = [algo_name, details, clusters,sil_score, db_index, ch_score]\n",
        "  for i,detail in enumerate(clustering_report):\n",
        "    clustering_report[detail].append(data[i])"
      ],
      "metadata": {
        "id": "Y2uM10SMUwYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DBSCAN"
      ],
      "metadata": {
        "id": "xUiM7IayVsYT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DBSCAN stands for Density-Based Spatial Clustering of Applications with Noise.It groups â€˜densely groupedâ€™ data points into a single cluster. It can identify clusters in large spatial datasets by looking at the local density of the data points. The most exciting feature of DBSCAN clustering is that it is robust to outliers. It also does not require the number of clusters to be told beforehand, unlike K-Means, where we have to specify the number of centroids.\n",
        "\n",
        "DBSCAN requires only two parameters: epsilon and minPoints. Epsilon is the radius of the circle to be created around each data point to check the density and minPoints is the minimum number of data points required inside that circle for that data point to be classified as a Core point.\n",
        "\n",
        "FITTING AND PREDICTING DATA WITH DBSCAN with default values"
      ],
      "metadata": {
        "id": "9F9j0s0BVw7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dbscan = DBSCAN().fit(X)\n",
        "preds = dbscan.labels_"
      ],
      "metadata": {
        "id": "tkOQZzHqVpiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CALCULATING SILHOUETTE SCORES"
      ],
      "metadata": {
        "id": "meKoxpKrV4yJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "silhouette_score(X,preds)"
      ],
      "metadata": {
        "id": "xW3m86EgV1WQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "davies_bouldin_score(X,preds)"
      ],
      "metadata": {
        "id": "k6FhLrcQV8mW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calinski_harabasz_score(X,preds)"
      ],
      "metadata": {
        "id": "4xkZaBxXV8qy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NUMBER OF CLUSTERS"
      ],
      "metadata": {
        "id": "mJoc_2a9WFja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.Series(preds).nunique()"
      ],
      "metadata": {
        "id": "SqzrSmHnV8x9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OBSERVATIONS\n",
        "DBSCAN clustered the data into 9 clusters with a silhouette score 0.4968"
      ],
      "metadata": {
        "id": "LkMaxQDiWiVJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "VISUALIZATION\n",
        "For visualization, we're taking three significant components using PCA and plotting them 2 the front,side and top view of the 3d space. This way we can see how the clusters are arranged spatially"
      ],
      "metadata": {
        "id": "vBQiWhrzW18l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = pd.DataFrame(preds, columns = ['Labels'])"
      ],
      "metadata": {
        "id": "m_id2O5lV810"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dimensionality reduction using PCA\n",
        "pca = PCA(n_components = 3)\n",
        "     "
      ],
      "metadata": {
        "id": "npAUqpkKW8bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reduced = pd.DataFrame(pca.fit_transform(X), columns = ['Dimension 1', 'Dimension 2', 'Dimension 3'])\n",
        "dbscan_cluster = pd.concat([reduced, predictions], axis=1)"
      ],
      "metadata": {
        "id": "yvxZD_C2W9Zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#setting colors for all visualisations\n",
        "hundred_colors = \" #000000 dimgray #696969 silver #c0c0c0 darkslategray #2f4f4f darkolivegreen #556b2f olivedrab #6b8e23 sienna #a0522d seagreen #2e8b57 forestgreen #228b22 maroon2 #7f0000 midnightblue #191970 darkgreen #006400 olive #808000 darkslateblue #483d8b firebrick #b22222 lightslategray #778899 mediumseagreen #3cb371 rosybrown #bc8f8f rebeccapurple #663399 darkgoldenrod #b8860b darkkhaki #bdb76b darkcyan #008b8b steelblue #4682b4 chocolate #d2691e yellowgreen #9acd32 darkblue #00008b indigo #4b0082 limegreen #32cd32 darkseagreen #8fbc8f darkmagenta #8b008b maroon3 #b03060 mediumturquoise #48d1cc mediumaquamarine #66cdaa darkorchid #9932cc orangered #ff4500 orange #ffa500 gold #ffd700 yellow #ffff00 mediumvioletred #c71585 mediumblue #0000cd burlywood #deb887 chartreuse #7fff00 lime #00ff00 mediumorchid #ba55d3 springgreen #00ff7f darksalmon #e9967a crimson #dc143c aqua #00ffff deepskyblue #00bfff sandybrown #f4a460 mediumpurple #9370db blue #0000ff purple3 #a020f0 lightcoral #f08080 greenyellow #adff2f tomato #ff6347 thistle #d8bfd8 fuchsia #ff00ff palevioletred #db7093 khaki #f0e68c laserlemon #ffff54 cornflower #6495ed plum #dda0dd lightgreen #90ee90 lightblue #add8e6 deeppink #ff1493 mediumslateblue #7b68ee violet #ee82ee lightskyblue #87cefa aquamarine #7fffd4 lightgoldenrod #fafad2 hotpink #ff69b4 mistyrose #ffe4e1 lightcyan #e0ffff pink #ffc0cb\"\n",
        "colors = [color for color in hundred_colors.split(' ') if '#' in color]\n",
        "\n",
        "#looping to shuffle colors\n",
        "np.random.seed(151203)\n",
        "shuffled = []\n",
        "while len(colors)!=0:\n",
        "  a = np.random.choice(colors)\n",
        "  shuffled.append(a)\n",
        "  colors.remove(a)"
      ],
      "metadata": {
        "id": "vWJT1Q7PW__5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gyr = shuffled[:pd.Series(preds).nunique()]"
      ],
      "metadata": {
        "id": "SAPvrcSQXCsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.color_palette(gyr)"
      ],
      "metadata": {
        "id": "Ygbab61WXFUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,15))\n",
        "plt.suptitle(\"3 Dimensional PCA Cluster Visualizations\", fontsize=18, y=0.95)\n",
        "\n",
        "plt.subplot(2,2,1)\n",
        "plt.title('TOP VIEW')\n",
        "sns.scatterplot(x = 'Dimension 1', y = 'Dimension 2', data = dbscan_cluster,palette = sns.color_palette(gyr), hue = 'Labels', legend=False)\n",
        "\n",
        "plt.subplot(2,2,2)\n",
        "plt.title('SIDE VIEW')\n",
        "sns.scatterplot(x = 'Dimension 3', y = 'Dimension 2',  data = dbscan_cluster,palette = sns.color_palette(gyr), hue = 'Labels', legend=False)\n",
        "\n",
        "plt.subplot(2,2,3) \n",
        "plt.title('FRONT VIEW')\n",
        "palette = {0: \"C0\", 1: \"C1\", 2: \"C2\", 3: \"C3\", 4: \"C4\", 5: \"C5\",\n",
        "           6: \"C6\", 7: \"C7\", 8: \"C8\", 9: \"C9\", 10: \"C10\", 11: \"C11\",\n",
        "           12: \"C12\", 13: \"C13\", 14: \"C14\", 15: \"C15\", 16: \"C16\"}\n",
        "sns.scatterplot(x = 'Dimension 1', y = 'Dimension 3', data = dbscan_cluster,palette = sns.color_palette(gyr), hue = 'Labels',legend=False)"
      ],
      "metadata": {
        "id": "QKowqIxeXGF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K-MEANS\n",
        "It is an iterative algorithm that divides the unlabeled dataset into k different clusters in such a way that each dataset belongs only one group that has similar propert\n",
        "\n",
        "Implementing K-means clustering and calculating the Silhouette scores for clusters of 2 to 25 for content based on Release Year, Ratings, Type and Duration\n",
        "\n",
        "A silhouette score of 1 means that the clusters are very dense and nicely separated. The score of 0 means that clusters are overlapping. The score of less than 0 means that data belonging to clusters may be wrong/incorrect. The silhouette plots can be used to select the most optimal value of the K"
      ],
      "metadata": {
        "id": "FG3gfk6nXUON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "silhouette_score_ = [ ]\n",
        "sum_of_sq_dist = {}\n",
        "range_n_clusters = [i for i in range(2,21)]\n",
        "for n_clusters in range_n_clusters:\n",
        "    clusterer = KMeans(n_clusters=n_clusters, n_init=10,random_state=10)\n",
        "    preds = clusterer.fit_predict(X)\n",
        "    centers = clusterer.cluster_centers_\n",
        "    sum_of_sq_dist[n_clusters] = clusterer.inertia_\n",
        "\n",
        "    score = silhouette_score(X, preds)\n",
        "    silhouette_score_.append([int(n_clusters) , round(score , 3)])"
      ],
      "metadata": {
        "id": "QOeEcObaXXuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Kmeans = pd.DataFrame(silhouette_score_ , columns = [\"No of Clusters\" , \"Silhouette Score\"])"
      ],
      "metadata": {
        "id": "E86kuMBjXcTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clusters vs Silhouette score"
      ],
      "metadata": {
        "id": "ixBv8-iGXlsm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(25,10))\n",
        "_ = sns.lineplot(x = Kmeans['No of Clusters'], y = Kmeans['Silhouette Score'])\n",
        "_ = plt.xticks(ticks = np.arange(0,22,1))"
      ],
      "metadata": {
        "id": "_g7SClpmXm_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Kmeans.sort_values(['Silhouette Score','No of Clusters'],ascending=False).head()"
      ],
      "metadata": {
        "id": "Mjf1TR8gXpth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "OBSERVATIONS\n",
        "The highest silhouette score is 0.460 produced with 8 Clusters using K Means Algorithm"
      ],
      "metadata": {
        "id": "3ePya3Y8X0U-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Elbow Method to determine the Optimal number of Clusters"
      ],
      "metadata": {
        "id": "GOzXre09X5kV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the graph for the sum of square distance values and Number of Clusters\n",
        "plt.figure(figsize=(25,8))\n",
        "sns.pointplot(x = list(sum_of_sq_dist.keys()), y = list(sum_of_sq_dist.values()))\n",
        "plt.xlabel('Number of Clusters(k)')\n",
        "plt.ylabel('Sum of Square Distances')\n",
        "plt.title('Elbow Method For Optimal k')\n",
        "plt.xticks(rotation = 90)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZFpZtTrSX1q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OBSERVATIONS\n",
        "From the above graph has plots of the sum squared inertia for K clusters trained on K means algorithm\n",
        "In this graph, we decide the optimal number of clusters by locating the elbow of the graph which is at 8 clusters"
      ],
      "metadata": {
        "id": "j0B_xUIXYASX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Displaying Silhouette plot for K means Clustering"
      ],
      "metadata": {
        "id": "0wBu2c9jYEn1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "range_n_clusters = [i for i in range(2,20)]\n",
        "silhouette_score_ = []\n",
        "for n_clusters in range_n_clusters:\n",
        "    # Create a subplot with 1 row and 2 columns\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "    fig.set_size_inches(18, 7)\n",
        "\n",
        "    # The 1st subplot is the silhouette plot\n",
        "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
        "    # lie within [-0.1, 1]\n",
        "    ax1.set_xlim([-0.1, 1])\n",
        "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
        "    # plots of individual clusters, to demarcate them clearly.\n",
        "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
        "\n",
        "    # Initialize the clusterer with n_clusters value and a random generator\n",
        "    # seed of 10 for reproducibility.\n",
        "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
        "    cluster_labels = clusterer.fit_predict(X)\n",
        "\n",
        "    # The silhouette_score gives the average value for all the samples.\n",
        "    # This gives a perspective into the density and separation of the formed\n",
        "    # clusters\n",
        "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
        "    print(\"For n_clusters =\", n_clusters,\n",
        "          \"The average silhouette_score is :\", silhouette_avg)\n",
        "\n",
        "    # Compute the silhouette scores for each sample\n",
        "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
        "\n",
        "    y_lower = 10\n",
        "    for i in range(n_clusters):\n",
        "        # Aggregate the silhouette scores for samples belonging to\n",
        "        # cluster i, and sort them\n",
        "        ith_cluster_silhouette_values = \\\n",
        "            sample_silhouette_values[cluster_labels == i]\n",
        "\n",
        "        ith_cluster_silhouette_values.sort()\n",
        "\n",
        "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "        y_upper = y_lower + size_cluster_i\n",
        "\n",
        "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
        "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
        "                          0, ith_cluster_silhouette_values,\n",
        "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
        "\n",
        "        # Label the silhouette plots with their cluster numbers at the middle\n",
        "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "\n",
        "        # Compute the new y_lower for next plot\n",
        "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
        "\n",
        "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
        "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
        "    ax1.set_ylabel(\"Cluster label\")\n",
        "\n",
        "    # The vertical line for average silhouette score of all the values\n",
        "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "\n",
        "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
        "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "\n",
        "    # 2nd Plot showing the actual clusters formed\n",
        "    colors = cm.nipy_spectral(cluster_labels.astype(float) /n_clusters)\n",
        "    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
        "                c=colors, edgecolor='k')\n",
        "\n",
        "    # Labeling the clusters\n",
        "    centers = clusterer.cluster_centers_\n",
        "    # Draw white circles at cluster centers\n",
        "    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
        "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
        "\n",
        "    \n",
        "\n",
        "    ax2.set_title(\"The visualization of the clustered data.\")\n",
        "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
        "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
        "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
        "                  \"with n_clusters = %d\" % n_clusters),\n",
        "                 fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PzedxFOfZd4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OBSERVATIONS\n",
        "From Silhouette Analysis and Elbow method, the optimal cluster is 8. This gives a clustering score of 0.389"
      ],
      "metadata": {
        "id": "BUX3EE_zaszs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VISUALIZATION for K = 8 clusters\n",
        "For visualization, we're taking three significant components using PCA and plotting them 2 the front,side and top view of the 3d space. This way we can see how the clusters are arranged spatially"
      ],
      "metadata": {
        "id": "RJCvlEFebC5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clusterer = KMeans(n_clusters= 8, n_init=10,random_state=10)\n",
        "preds = clusterer.fit_predict(X)"
      ],
      "metadata": {
        "id": "ZeiP-WIJZeup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "update_report('KMeans','Default' ,pd.Series(preds).nunique(), silhouette_score(X,preds), davies_bouldin_score(X,preds), calinski_harabasz_score(X,preds))\n",
        "     "
      ],
      "metadata": {
        "id": "O51JIUn2bKhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = pd.DataFrame(preds, columns = ['Clusters'])"
      ],
      "metadata": {
        "id": "hlHCLo6FbORZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dimensionality reduction using PCA\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components = 3)\n"
      ],
      "metadata": {
        "id": "_dUCXPL5bQiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reduced = pd.DataFrame(pca.fit_transform(X), columns = ['Dimension 1', 'Dimension 2', 'Dimension 3'])\n",
        "kmeans_cluster = pd.concat([reduced, predictions], axis=1)\n",
        "     "
      ],
      "metadata": {
        "id": "R_Rq2gEpbVd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#setting colors for all visualisations\n",
        "hundred_colors = \" #000000 dimgray #696969 silver #c0c0c0 darkslategray #2f4f4f darkolivegreen #556b2f olivedrab #6b8e23 sienna #a0522d seagreen #2e8b57 forestgreen #228b22 maroon2 #7f0000 midnightblue #191970 darkgreen #006400 olive #808000 darkslateblue #483d8b firebrick #b22222 lightslategray #778899 mediumseagreen #3cb371 rosybrown #bc8f8f rebeccapurple #663399 darkgoldenrod #b8860b darkkhaki #bdb76b darkcyan #008b8b steelblue #4682b4 chocolate #d2691e yellowgreen #9acd32 darkblue #00008b indigo #4b0082 limegreen #32cd32 darkseagreen #8fbc8f darkmagenta #8b008b maroon3 #b03060 mediumturquoise #48d1cc mediumaquamarine #66cdaa darkorchid #9932cc orangered #ff4500 orange #ffa500 gold #ffd700 yellow #ffff00 mediumvioletred #c71585 mediumblue #0000cd burlywood #deb887 chartreuse #7fff00 lime #00ff00 mediumorchid #ba55d3 springgreen #00ff7f darksalmon #e9967a crimson #dc143c aqua #00ffff deepskyblue #00bfff sandybrown #f4a460 mediumpurple #9370db blue #0000ff purple3 #a020f0 lightcoral #f08080 greenyellow #adff2f tomato #ff6347 thistle #d8bfd8 fuchsia #ff00ff palevioletred #db7093 khaki #f0e68c laserlemon #ffff54 cornflower #6495ed plum #dda0dd lightgreen #90ee90 lightblue #add8e6 deeppink #ff1493 mediumslateblue #7b68ee violet #ee82ee lightskyblue #87cefa aquamarine #7fffd4 lightgoldenrod #fafad2 hotpink #ff69b4 mistyrose #ffe4e1 lightcyan #e0ffff pink #ffc0cb\"\n",
        "colors = [color for color in hundred_colors.split(' ') if '#' in color]\n",
        "\n",
        "#looping to shuffle colors\n",
        "np.random.seed(151203)\n",
        "shuffled = []\n",
        "while len(colors)!=0:\n",
        "  a = np.random.choice(colors)\n",
        "  shuffled.append(a)\n",
        "  colors.remove(a)"
      ],
      "metadata": {
        "id": "8-tIFaLMbVh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gyr = shuffled[:pd.Series(preds).nunique()]"
      ],
      "metadata": {
        "id": "WRa0m6jObaXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.color_palette(gyr)"
      ],
      "metadata": {
        "id": "JCknEP0sbdNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import transforms\n",
        "plt.figure(figsize=(20,15))\n",
        "plt.suptitle(\"3 Dimensional PCA Cluster Visualizations\", fontsize=18, y=0.95)\n",
        "\n",
        "plt.subplot(2,2,1)\n",
        "plt.title('TOP VIEW')\n",
        "sns.scatterplot(x = 'Dimension 1', y = 'Dimension 2', data = kmeans_cluster, hue = 'Clusters',palette = sns.color_palette(gyr), legend=False\n",
        "                )\n",
        "\n",
        "plt.subplot(2,2,2)\n",
        "plt.title('SIDE VIEW')\n",
        "palette = {0: \"C0\", 1: \"C1\", 2: \"C2\", 3: \"C3\", 4: \"C4\", 5: \"C5\",\n",
        "           6: \"C6\", 7: \"C7\", 8: \"C8\", 9: \"C9\", 10: \"C10\", 11: \"C11\",\n",
        "           12: \"C12\", 13: \"C13\", 14: \"C14\", 15: \"C15\", 16: \"C16\"}\n",
        "sns.scatterplot(x = 'Dimension 3', y = 'Dimension 2', data = kmeans_cluster, hue = 'Clusters',palette = sns.color_palette(gyr), legend=False)\n",
        "\n",
        "plt.subplot(2,2,3) \n",
        "plt.title('FRONT VIEW')\n",
        "palette = {0: \"C0\", 1: \"C1\", 2: \"C2\", 3: \"C3\", 4: \"C4\", 5: \"C5\",\n",
        "           6: \"C6\", 7: \"C7\", 8: \"C8\", 9: \"C9\", 10: \"C10\", 11: \"C11\",\n",
        "           12: \"C12\", 13: \"C13\", 14: \"C14\", 15: \"C15\", 16: \"C16\"}\n",
        "sns.scatterplot(x = 'Dimension 1', y = 'Dimension 3', data = kmeans_cluster, hue = 'Clusters',palette = sns.color_palette(gyr))\n",
        "plt.legend(bbox_to_anchor=(1.5,1))"
      ],
      "metadata": {
        "id": "ZQKhfbbNbfrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HIERARCHICAL CLUSTERING\n",
        "Hierarchical clustering, also known as hierarchical cluster analysis, is an algorithm that groups similar objects into groups called clusters. The endpoint is a set of clusters, where each cluster is distinct from each other cluster, and the objects within each cluster are broadly similar to each other."
      ],
      "metadata": {
        "id": "TnzxwaZabm_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's import sch\n",
        "import scipy.cluster.hierarchy as sch\n",
        "hier_ward_euc = sch.linkage(X, method = 'ward', metric = 'euclidean')"
      ],
      "metadata": {
        "id": "HdQFVjbfbik4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DENDROGRAM"
      ],
      "metadata": {
        "id": "n87fpuLVbxy9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(30,15))\n",
        "dendrogram = sch.dendrogram(hier_ward_euc)\n",
        "plt.title('Dendrogram')\n",
        "plt.xlabel('Content')\n",
        "plt.ylabel('Euclidean Distances')\n",
        "plt.show() # find largest vertical distance we can make without crossing any other horizontal line"
      ],
      "metadata": {
        "id": "PGk3m1mZbuIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FROM DENDOGRAM, A DISTANCE OF 20-30 appears to have well defined trees.\n",
        "\n",
        "CHOOSING THE APPROPRIATE DISTANCE TO CLUSTER\n",
        "\n",
        "## Agglomerative Clustering"
      ],
      "metadata": {
        "id": "IjJEEQahcb5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clusters = []\n",
        "scores = []\n",
        "distances = []\n",
        "for i in np.arange(5,75,5):\n",
        "  hc = AgglomerativeClustering(n_clusters = None, distance_threshold = i, affinity = 'euclidean', linkage = 'ward')\n",
        "  y_hc = hc.fit_predict(X)\n",
        "  clusters.append(hc.labels_.max()+1)\n",
        "  if (hc.labels_.max()+1 >=2) :\n",
        "    scores.append(silhouette_score(X, y_hc))\n",
        "  else:\n",
        "    scores.append(1)\n",
        "  distances.append(i)\n",
        "\n",
        "hierarchical = pd.DataFrame({'Clusters': clusters, \n",
        "                             'Silhouette Scores': scores,\n",
        "                             'Distance': distances})\n",
        "     "
      ],
      "metadata": {
        "id": "BhyGTX6eb3Jw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DISTANCE vs SILHOUETTE SCORES GRAPH FOR HIERARCHICAL CLUSTERING"
      ],
      "metadata": {
        "id": "DzwhhZORcnyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize =(15,8))\n",
        "sns.lineplot(x = 'Distance', y = 'Silhouette Scores' , data = hierarchical)\n",
        "a = plt.title('Hierarchical Clustering: Distance vs Silhouette Scores')\n",
        "_ = plt.xticks(np.arange(5,75,1),rotation = 90)"
      ],
      "metadata": {
        "id": "yW2EbtcncnAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distance-Silhouette Score-Clusters Table for Hierarchical Clustering"
      ],
      "metadata": {
        "id": "FIMsHJh0cu1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hierarchical.sort_values('Silhouette Scores', ascending = False).head()"
      ],
      "metadata": {
        "id": "Kxc_dlSkcihJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OBSERVATIONS :\n",
        "Highest Silhouette Score of 0.498 achieved at distance = 20 with 8 clusters"
      ],
      "metadata": {
        "id": "yIMo_8W8c5kO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize =(30,15))\n",
        "plt.title(\"Dendrograms\")  \n",
        "dend = sch.dendrogram(sch.linkage(X, method='ward'))\n",
        "plt.axhline(y= 20, color='black', linestyle='--')"
      ],
      "metadata": {
        "id": "gP4pjOj5c2ap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VISUALIZATION for Distance = 20\n",
        "For visualization, we're taking three significant components using PCA and plotting them 2 the front,side and top view of the 3d space. This way we can see how the clusters are arranged spatially"
      ],
      "metadata": {
        "id": "uXcC0TAtdsZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hc = AgglomerativeClustering(n_clusters = None, distance_threshold = 20, affinity = 'euclidean', linkage = 'ward')\n",
        "preds = hc.fit_predict(X)"
      ],
      "metadata": {
        "id": "V1kpveWVdAoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "update_report('Hierarchical Agglomerative','Distance = 20' ,pd.Series(preds).nunique(), silhouette_score(X,preds), davies_bouldin_score(X,preds), calinski_harabasz_score(X,preds))"
      ],
      "metadata": {
        "id": "goCfDGVUdzj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = pd.DataFrame(preds, columns = ['Labels'])"
      ],
      "metadata": {
        "id": "gjUcL-ifdzoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dimensionality reduction using PCA\n",
        "pca = PCA(n_components = 3)"
      ],
      "metadata": {
        "id": "Gq7AhPKxeIyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reduced = pd.DataFrame(pca.fit_transform(X), columns = ['Dimension 1', 'Dimension 2', 'Dimension 3'])\n",
        "hier_cluster = pd.concat([reduced, predictions], axis=1)"
      ],
      "metadata": {
        "id": "ai6AdKfueI35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "colors = shuffled[:pd.Series(hc.labels_).nunique()]\n",
        "\n",
        "sns.color_palette(colors)"
      ],
      "metadata": {
        "id": "B7iBc-JEeOIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,15))\n",
        "plt.suptitle(\"3 Dimensional PCA Cluster Visualizations\", fontsize=18, y=0.95)\n",
        "\n",
        "plt.subplot(2,2,1)\n",
        "plt.title('TOP VIEW')\n",
        "sns.scatterplot(x = 'Dimension 1', y = 'Dimension 2', data = hier_cluster, hue = 'Labels',palette = sns.color_palette(colors), legend=False)\n",
        "\n",
        "plt.subplot(2,2,2)\n",
        "plt.title('SIDE VIEW')\n",
        "palette = {0: \"C0\", 1: \"C1\", 2: \"C2\", 3: \"C3\", 4: \"C4\", 5: \"C5\",\n",
        "           6: \"C6\", 7: \"C7\", 8: \"C8\", 9: \"C9\", 10: \"C10\", 11: \"C11\",\n",
        "           12: \"C12\", 13: \"C13\", 14: \"C14\", 15: \"C15\", 16: \"C16\"}\n",
        "sns.scatterplot(x = 'Dimension 3', y = 'Dimension 2',  data = hier_cluster, hue = 'Labels',palette = sns.color_palette(colors), legend=False)\n",
        "\n",
        "plt.subplot(2,2,3) \n",
        "plt.title('FRONT VIEW')\n",
        "palette = {0: \"C0\", 1: \"C1\", 2: \"C2\", 3: \"C3\", 4: \"C4\", 5: \"C5\",\n",
        "           6: \"C6\", 7: \"C7\", 8: \"C8\", 9: \"C9\", 10: \"C10\", 11: \"C11\",\n",
        "           12: \"C12\", 13: \"C13\", 14: \"C14\", 15: \"C15\", 16: \"C16\"}\n",
        "sns.scatterplot(x = 'Dimension 1', y = 'Dimension 3', data = hier_cluster, hue = 'Labels',palette = sns.color_palette(colors), legend=False)"
      ],
      "metadata": {
        "id": "o1_JkW99eQao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FINAL REPORT\n",
        "After optimizing and selecting the best number of clusters for the three models, the final report is displayed down below"
      ],
      "metadata": {
        "id": "iELkwJd4eXaj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(clustering_report)"
      ],
      "metadata": {
        "id": "7iMtyf-Figwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CONCLUSIONS**\n",
        "#1. Exploratory Data Analysis Conclusions\n",
        "Netflix began adding videos to the platform from 2008\n",
        "The streaming giant started aggressively adding movies and TV shows from 2017\n",
        "More movies are added as compared to TV shows\n",
        "There are almost twice as many movies as TV shows on Netflix.\n",
        "Most content on Netflix is rated for Mature Audiences and over 14 years old\n",
        "\n",
        "**Top Countries in Netflix are**:\n",
        "\n",
        "United States\n",
        "India\n",
        "United Kingdom\n",
        "Canada\n",
        "France\n",
        "\n",
        "**Top Genres in Netflix are:**\n",
        "\n",
        "1Drama\n",
        "2.Comedy 3.Documentary 4.Action and Adventure 5.Romance\n",
        "\n",
        "**Top Directors on Netflix are**:\n",
        "\n",
        "1Jan Suter\n",
        "2.Raul Campos 3.Marcus Raboy 4.Jay Karas 5.Cathy Garcia-Molina\n",
        "\n",
        "**Top Actors on Netflix are:**\n",
        "\n",
        "1Anupam Kher\n",
        "2.Shah Rukh Khan 3.Naseeruddin Shah 4.Om Puri 5.Akshay Kumar\n",
        "\n",
        "\n",
        "Most movies on Netflix have a duration range from 90 to 110 minutes\n",
        "Most TV shows on Netflix have a span of only one season\n",
        "\n",
        "#2. Analysis of Content produced in different countries\n",
        "\n",
        "Drama is the most produced genre in a lot of Non-English speaking countries\n",
        "Comedy is the most produced genre in English speaking countries like United States of America and United Kingdom and Canada\n",
        "Drama and Comedy are the most produced genres in the top countries with exceptions of Japan and South Korea\n",
        "Japan is the biggest producer of Anime. Anime is also the most produced in genre in Japan\n",
        "Most South Korean content are from the Romance genre*\n",
        "Documentaries are mainly produced in United Kingdom and United States of America\n",
        "\n",
        "#3. Has Netflix been focusing increasingly on TV showss as compared to movies\n",
        "From the first graph in the section, we can observe that seasons of TV shows signed vs the movies signed\n",
        "This distinction gives contacts as TV shows require recurring investment for each seasons. So the TV numbers have been increased in accordance to the seasons. As they were considered as one entity earlier\n",
        "We can observe that TV shows signed have been higher than movies in 2016. While the the movies signed have been higher, it is blatantly visible that the TV shows signed per year is catching up to the movies signed by the year\n",
        "\n",
        "From the second graph in the section, we can observe an estimation of seasonal TV show and Movie release date per each year. We make a general assumption that each show releases a season yearly which is often the case for most TV shows.\n",
        "There are certainly exemptions. Eg. Ozark was released in years 2017, 2018, 2020 and 2022.\n",
        "We can observe that TV releases outnumbering movies from 2010. It is a known fact that each show contains multiple episodes, hence we can confidently say that significant amount of video content is being enlisted under TV shows from 2010\n",
        "Years 2019, 2020 and 2021 see multi-episode TV seasons outnumbering movies signed per year\n",
        "\n",
        "#4.1 Topic Modeling\n",
        "Gensim Latent Drichlet was used to model textual data[description, genres, directors and cast] into topics\n",
        "Nine topics were decided to be the most suitable from comparing Coherence Scores for different topic numbers.\n",
        "The topics for each document will be used to classify the documents based on text as clustering can effectively handle so many values\n",
        "The table also contains how much a document belongs to each topic, thus allowing content to be a mixture of different themes with their own respective topic weightages\n",
        "By using this is as inputs, no data is lost in dimensionality reduction. The documents are classified based on their topics which is expected to be more sensible/insightful as an input\n",
        "The Topics and corresponding top words are given down below:\n",
        "\n",
        "Topic: 1\n",
        "\n",
        "drama\n",
        "comedy\n",
        "famili\n",
        "romance\n",
        "find\n",
        "young\n",
        "woman\n",
        "life\n",
        "love\n",
        "man\n",
        "\n",
        "Topic: 2\n",
        "\n",
        "drama\n",
        "horror\n",
        "thriller\n",
        "romance\n",
        "school\n",
        "comedy\n",
        "young\n",
        "teen\n",
        "student\n",
        "life\n",
        "\n",
        "Topic: 3\n",
        "\n",
        "actionadventure\n",
        "scififantasy\n",
        "drama\n",
        "sports\n",
        "anime\n",
        "team\n",
        "world\n",
        "kids\n",
        "power\n",
        "save\n",
        "\n",
        "Topic: 4\n",
        "\n",
        "comedy\n",
        "drama\n",
        "school\n",
        "family\n",
        "high\n",
        "new\n",
        "teen\n",
        "world\n",
        "student\n",
        "scififantasy\n",
        "\n",
        "Topic: 5\n",
        "\n",
        "crime\n",
        "drama\n",
        "thriller\n",
        "actionadventure\n",
        "murder\n",
        "investig\n",
        "cop\n",
        "polic\n",
        "detect\n",
        "drug\n",
        "\n",
        "Topic: 6\n",
        "\n",
        "documentary\n",
        "documentari\n",
        "music\n",
        "seri\n",
        "stori\n",
        "explor\n",
        "life\n",
        "film\n",
        "world\n",
        "follow\n",
        "\n",
        "Topic: 7\n",
        "\n",
        "comedy\n",
        "drama\n",
        "romance\n",
        "new\n",
        "find\n",
        "year\n",
        "life\n",
        "love\n",
        "get\n",
        "friend\n",
        "\n",
        "Topic: 8\n",
        "\n",
        "drama\n",
        "comedy\n",
        "romance\n",
        "love\n",
        "famili\n",
        "man\n",
        "young\n",
        "woman\n",
        "life\n",
        "two\n",
        "\n",
        "Topic: 9\n",
        "\n",
        "comedy\n",
        "kids\n",
        "reality\n",
        "friend\n",
        "family\n",
        "comedi\n",
        "special\n",
        "show\n",
        "stand\n",
        "comedian\n",
        "\n",
        "#4.2 Clustering Results\n",
        "\n",
        "# DBSCAN\n",
        "DBSCAN clustered the data into 8 clusters with a silhouette score 0.4664\n",
        "The model also had a Davies-Bouding Index of 1.6201 and Calinski-Harbaz Score of 2510.7\n",
        "\n",
        "#K-MEANS\n",
        "From the elbow graph and silhouette analysis, the best model has a silhouette score of 0.4686 produced with 8 Clusters using K Means Algorithm\n",
        "The model also had a Davies-Bouding Index of 0.8895 and Calinski-Harbaz Score of 2901.84\n",
        "\n",
        "#Hierarchical Clustering\n",
        "Using dendograms and comparing various distance thresholds, a distance of 20 produced the highest silhouette score of 0.4687 with 8 clusters\n",
        "The model also had a Davies-Bouding Index of 0.8886 and Calinski-Harbaz Score of 2900.28\n"
      ],
      "metadata": {
        "id": "ZvPgDN1yes4g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## With the above results we can conclude that K-Means and Hierarchical clustering have the best results with 8 clusters"
      ],
      "metadata": {
        "id": "m1NvqMQCgQae"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GKsZYpQZedUA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}